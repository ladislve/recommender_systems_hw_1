{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline Evaluation Framework\n",
    "**Objective:** Define a consistent protocol to simulate real-world usage\n",
    "**Strategy:**\n",
    "1.  **Split:** Temporal User Split (80/10/10). We sort each user's history by time to respect causality (predicting the next movie they will watch)\n",
    "2.  **Metrics:** A mix of Accuracy (NDCG), Classification (Recall/Precision), and Diversity (Coverage) to detect popularity bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_DIR = \"../data/ml-1m\"\n",
    "\n",
    "ratings = pd.read_csv(\n",
    "    os.path.join(DATA_DIR, \"ratings.dat\"),\n",
    "    sep=\"::\",\n",
    "    engine=\"python\",\n",
    "    names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"],\n",
    "    encoding=\"latin-1\"\n",
    ")\n",
    "\n",
    "movies = pd.read_csv(\n",
    "    os.path.join(DATA_DIR, \"movies.dat\"),\n",
    "    sep=\"::\",\n",
    "    engine=\"python\",\n",
    "    names=[\"item_id\", \"title\", \"genres\"],\n",
    "    encoding=\"latin-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use per-user temporal split to ensure that every user in the test set has a history in the training set. This avoids cold-start user problem for evaluation purposes, so that we could focus on algorithms themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 797,758 (79.8%)\n",
      "Val: 97,383 (9.7%)\n",
      "Test: 105,068 (10.5%)\n",
      "\n",
      "Train users: 6,040\n",
      "Val users: 6,040\n",
      "Test users: 6,040\n",
      "\n",
      "Train items: 3,666\n",
      "Val items: 3,360\n",
      "Test items: 3,445\n"
     ]
    }
   ],
   "source": [
    "def per_user_temporal_split(df, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    for user_id, user_df in df.groupby('user_id'):\n",
    "        user_sorted = user_df.sort_values('timestamp')\n",
    "        n = len(user_sorted)\n",
    "        \n",
    "        train_end = int(n * train_ratio)\n",
    "        val_end = train_end + int(n * val_ratio)\n",
    "        \n",
    "        train_data.append(user_sorted.iloc[:train_end])\n",
    "        val_data.append(user_sorted.iloc[train_end:val_end])\n",
    "        test_data.append(user_sorted.iloc[val_end:])\n",
    "    \n",
    "    train_df = pd.concat(train_data, ignore_index=True)\n",
    "    val_df = pd.concat(val_data, ignore_index=True)\n",
    "    test_df = pd.concat(test_data, ignore_index=True)\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train, val, test = per_user_temporal_split(ratings)\n",
    "\n",
    "print(f\"Train: {len(train):,} ({len(train)/len(ratings)*100:.1f}%)\")\n",
    "print(f\"Val: {len(val):,} ({len(val)/len(ratings)*100:.1f}%)\")\n",
    "print(f\"Test: {len(test):,} ({len(test)/len(ratings)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTrain users: {train['user_id'].nunique():,}\")\n",
    "print(f\"Val users: {val['user_id'].nunique():,}\")\n",
    "print(f\"Test users: {test['user_id'].nunique():,}\")\n",
    "\n",
    "print(f\"\\nTrain items: {train['item_id'].nunique():,}\")\n",
    "print(f\"Val items: {val['item_id'].nunique():,}\")\n",
    "print(f\"Test items: {test['item_id'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All sets have the same amount of users\n",
    "* There is small discrepancy in amount of items. It means that some items appear only in the future (cold start items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cold Start items check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New items in val (not in train): 18\n",
      "New items in test (not in train): 27\n",
      "Val interactions with new items: 20\n",
      "Test interactions with new items: 36\n"
     ]
    }
   ],
   "source": [
    "train_items = set(train['item_id'].unique())\n",
    "val_items = set(val['item_id'].unique())\n",
    "test_items = set(test['item_id'].unique())\n",
    "\n",
    "new_items_val = val_items - train_items\n",
    "new_items_test = test_items - train_items\n",
    "\n",
    "print(f\"New items in val (not in train): {len(new_items_val)}\")\n",
    "print(f\"New items in test (not in train): {len(new_items_test)}\")\n",
    "print(f\"Val interactions with new items: {val[val['item_id'].isin(new_items_val)].shape[0]}\")\n",
    "print(f\"Test interactions with new items: {test[test['item_id'].isin(new_items_test)].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are 27 movies in the test set that model would not see during traing. Pure Collaborative filtering will fail for these 36 interactions (it can't recommend id it had never seen). Thus we need Content-based approach to handle such cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics definition\n",
    "Down below are metrics we found to be the most suitable for our cause\n",
    "\n",
    "* NDCG: measures ranking quality. Penalizes putting relevant items low in the list\n",
    "* Precision/Recall @K: treats recommendation problem as classification one - out of retrieved items, did we find good ones? We set threshold = 4.0, because we only care if user liked recommendation, not just rated it\n",
    "* Coverage: what percentage of available items is the model recommending (to prevent recommending only popular items)\n",
    "* Popularity bias : measures average popularity of recommended items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(relevance_scores, k):\n",
    "    relevance_scores = np.array(relevance_scores)[:k]\n",
    "    if relevance_scores.size == 0:\n",
    "        return 0.0\n",
    "    gains = 2 ** relevance_scores - 1\n",
    "    discounts = np.log2(np.arange(2, relevance_scores.size + 2))\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "def ndcg_at_k(predictions, ground_truth, k=10):\n",
    "    if not predictions or not ground_truth:\n",
    "        return 0.0\n",
    "    \n",
    "    relevance = [ground_truth.get(item_id, 0) for item_id, _ in predictions[:k]]\n",
    "    \n",
    "    ideal_relevance = sorted(ground_truth.values(), reverse=True)\n",
    "    \n",
    "    dcg = dcg_at_k(relevance, k)\n",
    "    idcg = dcg_at_k(ideal_relevance, k)\n",
    "    \n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def recall_at_k(predictions, ground_truth, k=10, threshold=4.0):\n",
    "    if not predictions or not ground_truth:\n",
    "        return 0.0\n",
    "    \n",
    "    relevant_items = {item for item, rating in ground_truth.items() if rating >= threshold}\n",
    "    \n",
    "    if len(relevant_items) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    predicted_items = {item_id for item_id, _ in predictions[:k]}\n",
    "    \n",
    "    hits = len(predicted_items & relevant_items)\n",
    "    return hits / len(relevant_items)\n",
    "\n",
    "def precision_at_k(predictions, ground_truth, k=10, threshold=4.0):\n",
    "    if not predictions or not ground_truth:\n",
    "        return 0.0\n",
    "    \n",
    "    relevant_items = {item for item, rating in ground_truth.items() if rating >= threshold}\n",
    "    predicted_items = {item_id for item_id, _ in predictions[:k]}\n",
    "    \n",
    "    if len(predicted_items) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    hits = len(predicted_items & relevant_items)\n",
    "    return hits / len(predicted_items)\n",
    "\n",
    "def coverage(all_predictions, item_catalog):\n",
    "    recommended_items = set()\n",
    "    for preds in all_predictions:\n",
    "        recommended_items.update([item_id for item_id, _ in preds])\n",
    "    \n",
    "    return len(recommended_items) / len(item_catalog) if len(item_catalog) > 0 else 0.0\n",
    "\n",
    "def popularity_bias(all_predictions, item_popularity):\n",
    "    all_recommended = []\n",
    "    for preds in all_predictions:\n",
    "        all_recommended.extend([item_id for item_id, _ in preds])\n",
    "    \n",
    "    if not all_recommended:\n",
    "        return 0.0\n",
    "    \n",
    "    avg_pop = np.mean([item_popularity.get(item, 0) for item in all_recommended])\n",
    "    return avg_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator class\n",
    "We calculate metrics per user, then average them to prevent domanation of power users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommenderEvaluator:    \n",
    "    def __init__(self, train_df, test_df, k_values=[5, 10, 20], relevance_threshold=4.0):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.k_values = k_values\n",
    "        self.relevance_threshold = relevance_threshold\n",
    "        \n",
    "        self.ground_truth = self.build_ground_truth()\n",
    "        \n",
    "        self.item_catalog = set(train_df['item_id'].unique())\n",
    "        self.item_popularity = train_df.groupby('item_id').size().to_dict()\n",
    "        \n",
    "    def build_ground_truth(self):\n",
    "        ground_truth = defaultdict(dict)\n",
    "        for _, row in self.test_df.iterrows():\n",
    "            ground_truth[row['user_id']][row['item_id']] = row['rating']\n",
    "        return dict(ground_truth)\n",
    "    \n",
    "    def evaluate_model(self, model, model_name=\"Model\"):\n",
    "        results = defaultdict(list)\n",
    "        all_predictions = []\n",
    "        \n",
    "        test_users = list(self.ground_truth.keys())\n",
    "        \n",
    "        for user_id in test_users:\n",
    "            gt = self.ground_truth[user_id]\n",
    "            \n",
    "            max_k = max(self.k_values)\n",
    "            predictions = model.predict_for_user(user_id, k=max_k)\n",
    "            all_predictions.append(predictions)\n",
    "            \n",
    "            for k in self.k_values:\n",
    "                results[f'NDCG@{k}'].append(ndcg_at_k(predictions, gt, k))\n",
    "                results[f'Recall@{k}'].append(recall_at_k(predictions, gt, k, self.relevance_threshold))\n",
    "                results[f'Precision@{k}'].append(precision_at_k(predictions, gt, k, self.relevance_threshold))\n",
    "        \n",
    "        metrics = {}\n",
    "        for metric_name, values in results.items():\n",
    "            metrics[metric_name] = np.mean(values)\n",
    "        \n",
    "        metrics['Coverage'] = coverage(all_predictions, self.item_catalog)\n",
    "        metrics['Popularity_Bias'] = popularity_bias(all_predictions, self.item_popularity)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def print_metrics(self, metrics, model_name=\"Model\"):\n",
    "        print(f\"{model_name} - Evaluation results\")\n",
    "        \n",
    "        print(\"Ranking metrics:\")\n",
    "        for k in self.k_values:\n",
    "            print(f\"NDCG@{k:2d}: {metrics[f'NDCG@{k}']:.4f}\")\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        \n",
    "        print(\"Relevance metrics (threshold={:.1f}):\".format(self.relevance_threshold))\n",
    "        for k in self.k_values:\n",
    "            print(f\"Recall@{k:2d}: {metrics[f'Recall@{k}']:.4f}\")\n",
    "            print(f\"Precision@{k:2d}: {metrics[f'Precision@{k}']:.4f}\")\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        \n",
    "        print(f\"Diversity metrics:\")\n",
    "        print(f\"Coverage: {metrics['Coverage']:.4f}\")\n",
    "        print(f\"Popularity bias: {metrics['Popularity_Bias']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "\n",
    "train.to_csv(\"../data/processed/train.csv\", index=False)\n",
    "val.to_csv(\"../data/processed/val.csv\", index=False)\n",
    "test.to_csv(\"../data/processed/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random baseline\n",
    "Simulate random guess (Monte carlo simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random baseline - Evaluation results\n",
      "Ranking metrics:\n",
      "NDCG@ 5: 0.0022\n",
      "NDCG@10: 0.0030\n",
      "NDCG@20: 0.0039\n",
      "\n",
      "\n",
      "Relevance metrics (threshold=4.0):\n",
      "Recall@ 5: 0.0011\n",
      "Precision@ 5: 0.0022\n",
      "Recall@10: 0.0028\n",
      "Precision@10: 0.0024\n",
      "Recall@20: 0.0052\n",
      "Precision@20: 0.0024\n",
      "\n",
      "\n",
      "Diversity metrics:\n",
      "Coverage: 1.0000\n",
      "Popularity bias: 217.29\n"
     ]
    }
   ],
   "source": [
    "class RandomRecommender:\n",
    "    def __init__(self, train_df):\n",
    "        self.items = train_df['item_id'].unique()\n",
    "    \n",
    "    def predict_for_user(self, user_id, k=10):\n",
    "        selected = np.random.choice(self.items, size=min(k, len(self.items)), replace=False)\n",
    "        scores = np.random.rand(len(selected))\n",
    "        return list(zip(selected, scores))\n",
    "\n",
    "evaluator = RecommenderEvaluator(train, test, k_values=[5, 10, 20])\n",
    "random_model = RandomRecommender(train)\n",
    "metrics = evaluator.evaluate_model(random_model, \"Random baseline\")\n",
    "evaluator.print_metrics(metrics, \"Random baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NDCG@10 = 0.003 : effectively zero. Thus the task is not trivial\n",
    "* Coverage = 100% : random guesses naturally covers all the catalog, thus high diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popularity baseline\n",
    "Checking how effective is the strategy of recommending only the most popular items is, since we know that 50% of ratings come from 500 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Popularity baseline - Evaluation results\n",
      "Ranking metrics:\n",
      "NDCG@ 5: 0.0182\n",
      "NDCG@10: 0.0223\n",
      "NDCG@20: 0.0326\n",
      "\n",
      "\n",
      "Relevance metrics (threshold=4.0):\n",
      "Recall@ 5: 0.0128\n",
      "Precision@ 5: 0.0167\n",
      "Recall@10: 0.0240\n",
      "Precision@10: 0.0161\n",
      "Recall@20: 0.0498\n",
      "Precision@20: 0.0170\n",
      "\n",
      "\n",
      "Diversity metrics:\n",
      "Coverage: 0.0055\n",
      "Popularity bias: 2328.40\n"
     ]
    }
   ],
   "source": [
    "class PopularityRecommender:\n",
    "    def __init__(self, train_df):\n",
    "        self.popularity = train_df.groupby('item_id').size().sort_values(ascending=False)\n",
    "        self.popular_items = list(self.popularity.index)\n",
    "    \n",
    "    def predict_for_user(self, user_id, k=10):\n",
    "        items = self.popular_items[:k]\n",
    "        scores = [self.popularity[item] for item in items]\n",
    "        return list(zip(items, scores))\n",
    "\n",
    "pop_model = PopularityRecommender(train)\n",
    "pop_metrics = evaluator.evaluate_model(pop_model, \"Popularity baseline\")\n",
    "evaluator.print_metrics(pop_metrics, \"Popularity baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NDCG@10 has jumped significantly to 0.022 from 0.003, meaning that popularity alone is responsible for huge chunk of user behaviour. Any model should beat this threshold to be considered useful\n",
    "* The trade-off is that model achieves good accuracy by ignoring 99.5% of items (coverage 0.0055). \n",
    "* As expected average amount of ratings for recommended items in this approach is high (2300 on average)\n",
    "\n",
    "Our future goal is to increase NDCG, while increasing coverage compared to this baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
