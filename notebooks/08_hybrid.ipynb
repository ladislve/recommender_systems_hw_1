{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hybrid Recommender Systems**\n",
    "\n",
    "Combine heterogeneous signals into a Hybrid Recommender System. Implement at least one hybrid strategy (e.g., weighted blending, candidate generation + reranking), and explicitly discuss the optimization trade-offs, why the performance improves or degrades, and who benefits from the hybrid structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combined BPR-MF with an Enhanced CB recommender. We implemented two distinct architectural strategies:\n",
    "1. core-level fusion where predictions are normalized and combined using a tuned parameter $\\alpha$.\n",
    "2. two-stage pipeline where BPR retrieves the top 100 candidates, and the CB model reranks them for the final top 10.\n",
    "\n",
    "We then evaluated the models on the test set and performed a segmented analysis on \"Cold\" vs \"Warm\" users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 797,758 | Val: 97,383 | Test: 105,068\n",
      "Loaded 1,000,209 ratings\n",
      "Loaded 3,883 movies\n",
      "Loaded 6,040 users\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "sys.path.append(\"../src/evaluation\")\n",
    "sys.path.append(\"../src/models\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import njit\n",
    "from collections import defaultdict\n",
    "from utils.data_loader import MovieLensDataLoader\n",
    "from evaluator import RecommenderEvaluator\n",
    "from content_based import build_tfidf_features, EnhancedContentBasedRecommender\n",
    "from numba import prange\n",
    "\n",
    "loader = MovieLensDataLoader()\n",
    "train, val, test = loader.load_splits()\n",
    "_, movies, _ = loader.load_raw_data()\n",
    "\n",
    "n_users = train['user_id'].max() + 1\n",
    "n_items = train['item_id'].max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from 07_bpr\n",
    "@njit(fastmath=True)\n",
    "def bpr_epoch(user_ids, pos_items_flat, pos_offsets, pos_counts,\n",
    "              n_items, user_factors, item_factors, user_bias, item_bias,\n",
    "              lr, reg, n_samples):\n",
    "    \n",
    "    loss = 0.0\n",
    "    n_users = len(pos_offsets)\n",
    "    for _ in range(n_samples):\n",
    "        u = np.random.randint(0, n_users)\n",
    "        uid = user_ids[u]\n",
    "        count = pos_counts[u]\n",
    "        if count == 0:\n",
    "            continue\n",
    "        offset = pos_offsets[u]\n",
    "        pi = pos_items_flat[offset + np.random.randint(0, count)]\n",
    "        ni = np.random.randint(0, n_items)\n",
    "        attempts = 0\n",
    "        while attempts < 10:\n",
    "            found = False\n",
    "            for k in range(count):\n",
    "                if pos_items_flat[offset + k] == ni:\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                break\n",
    "            ni = np.random.randint(0, n_items)\n",
    "            attempts += 1\n",
    "        x_ui = user_bias[uid] + item_bias[pi] + np.dot(user_factors[uid], item_factors[pi])\n",
    "        x_uj = user_bias[uid] + item_bias[ni] + np.dot(user_factors[uid], item_factors[ni])\n",
    "        x_uij = x_ui - x_uj\n",
    "        sig = 1.0 / (1.0 + np.exp(x_uij))\n",
    "        loss += np.log(1.0 / (1.0 + np.exp(-x_uij)) + 1e-10)\n",
    "        user_factors[uid] += lr * (sig * (item_factors[pi] - item_factors[ni]) - reg * user_factors[uid])\n",
    "        item_factors[pi] += lr * (sig * user_factors[uid] - reg * item_factors[pi])\n",
    "        item_factors[ni] += lr * (-sig * user_factors[uid] - reg * item_factors[ni])\n",
    "        item_bias[pi] += lr * (sig - reg * item_bias[pi])\n",
    "        item_bias[ni] += lr * (-sig - reg * item_bias[ni])\n",
    "    return loss / n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from 07_bpr\n",
    "class BPRMF:\n",
    "    def __init__(self, n_users, n_items, n_factors=64, lr=0.01, reg=0.001, n_epochs=40):\n",
    "        self.n_users, self.n_items = n_users, n_items\n",
    "        self.n_factors, self.lr, self.reg, self.n_epochs = n_factors, lr, reg, n_epochs\n",
    "        np.random.seed(42)\n",
    "        self.user_factors = np.random.normal(0, 0.01, (n_users, n_factors))\n",
    "        self.item_factors = np.random.normal(0, 0.01, (n_items, n_factors))\n",
    "        self.user_bias = np.zeros(n_users)\n",
    "        self.item_bias = np.zeros(n_items)\n",
    "    \n",
    "    def fit(self, user_pos_items):\n",
    "        uids, pf, offs, cnts = [], [], [], []\n",
    "        offset = 0\n",
    "        for uid in range(self.n_users):\n",
    "            items = list(user_pos_items.get(uid, []))\n",
    "            uids.append(uid); offs.append(offset); cnts.append(len(items))\n",
    "            pf.extend(items); offset += len(items)\n",
    "        uids, pf = np.array(uids, dtype=np.int64), np.array(pf, dtype=np.int64)\n",
    "        offs, cnts = np.array(offs, dtype=np.int64), np.array(cnts, dtype=np.int64)\n",
    "        n_samples = len(pf) * 5\n",
    "        for ep in range(self.n_epochs):\n",
    "            loss = bpr_epoch(uids, pf, offs, cnts, self.n_items,\n",
    "                           self.user_factors, self.item_factors,\n",
    "                           self.user_bias, self.item_bias, self.lr, self.reg, n_samples)\n",
    "            if ep % 10 == 0:\n",
    "                print(f\"BPR уpoch {ep:3d}: loss = {loss:.4f}\")\n",
    "        print(f\"BPR уpoch {self.n_epochs-1:3d}: loss = {loss:.4f}\")\n",
    "    \n",
    "    def score_all_items(self, user_id):\n",
    "        if user_id >= self.n_users:\n",
    "            return np.zeros(self.n_items)\n",
    "        return self.item_bias + self.item_factors @ self.user_factors[user_id]\n",
    "    \n",
    "    def predict_for_user(self, user_id, k=10, train_df=None):\n",
    "        scores = self.score_all_items(user_id)\n",
    "        if train_df is not None:\n",
    "            seen = train_df[train_df['user_id'] == user_id]['item_id'].values\n",
    "            scores[seen] = -np.inf\n",
    "        top_k = np.argpartition(scores, -k)[-k:]\n",
    "        top_k = top_k[np.argsort(scores[top_k])[::-1]]\n",
    "        return [(int(i), float(scores[i])) for i in top_k if scores[i] > -np.inf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPR уpoch   0: loss = -0.3193\n",
      "BPR уpoch  10: loss = -0.1372\n",
      "BPR уpoch  20: loss = -0.0696\n",
      "BPR уpoch  30: loss = -0.0515\n",
      "BPR уpoch  39: loss = -0.0436\n"
     ]
    }
   ],
   "source": [
    "# training BPR-MF (our best collaborative ranking model)\n",
    "THRESHOLD = 4.0\n",
    "pos_df = train[train['rating'] >= THRESHOLD]\n",
    "user_pos_items = pos_df.groupby('user_id')['item_id'].apply(set).to_dict()\n",
    "\n",
    "bpr = BPRMF(n_users, n_items, n_factors=64, lr=0.01, reg=0.001, n_epochs=40)\n",
    "bpr.fit(user_pos_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6040"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training enhanced Content-Based model (TF-IDF on movie genres with a popularity weight)\n",
    "tfidf_matrix, feature_names = build_tfidf_features(movies)\n",
    "cb = EnhancedContentBasedRecommender(tfidf_matrix, popularity_weight=0.3, min_ratings_threshold=5)\n",
    "cb.fit(train)\n",
    "len(cb.user_profiles)\n",
    "\n",
    "# both models are trained using the same data to prevent data leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hybrid strategy A: Weighted Blending**\n",
    "\n",
    "Linear combination of scores. Since BPR outputs unbounded dot-product scores and the CB model outputs cosine similarities (0 to 1), we must apply Min-Max normalization to both before blending. The formula is: `Score = α * BPR_norm + (1 - α) * CB_norm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedBlendHybrid:\n",
    "\n",
    "    def __init__(self, bpr_model, cb_model, alpha=0.7, n_items=None):\n",
    "        self.bpr = bpr_model\n",
    "        self.cb = cb_model\n",
    "        self.alpha = alpha\n",
    "        self.n_items = n_items\n",
    "    \n",
    "    def _normalize(self, scores):\n",
    "        mask = scores > -np.inf\n",
    "        if mask.sum() == 0:\n",
    "            return scores\n",
    "        valid = scores[mask]\n",
    "        mn, mx = valid.min(), valid.max()\n",
    "        if mx - mn < 1e-10:\n",
    "            scores[mask] = 0.5\n",
    "        else:\n",
    "            scores[mask] = (valid - mn) / (mx - mn)\n",
    "        return scores\n",
    "    \n",
    "    def predict_for_user(self, user_id, k=10, train_df=None):\n",
    "        bpr_scores = self.bpr.score_all_items(user_id).copy()\n",
    "        \n",
    "        if user_id in self.cb.user_profiles:\n",
    "            profile = self.cb.user_profiles[user_id]\n",
    "            cb_scores = self.cb.normalized_features @ profile\n",
    "            cb_scores = (1 - self.cb.popularity_weight) * cb_scores + \\\n",
    "                        self.cb.popularity_weight * self.cb.popularity_scores\n",
    "        else:\n",
    "            cb_scores = np.zeros(self.n_items)\n",
    "        \n",
    "        if train_df is not None:\n",
    "            seen = train_df[train_df['user_id'] == user_id]['item_id'].values\n",
    "            bpr_scores[seen] = -np.inf\n",
    "            cb_scores[seen] = -np.inf\n",
    "        \n",
    "        bpr_norm = self._normalize(bpr_scores)\n",
    "        cb_norm = self._normalize(cb_scores)\n",
    "        \n",
    "        final = self.alpha * bpr_norm + (1 - self.alpha) * cb_norm\n",
    "        \n",
    "        final[bpr_scores == -np.inf] = -np.inf\n",
    "        \n",
    "        top_k = np.argpartition(final, -k)[-k:]\n",
    "        top_k = top_k[np.argsort(final[top_k])[::-1]]\n",
    "        return [(int(i), float(final[i])) for i in top_k if final[i] > -np.inf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hybrid strategy A: Candidate Generation + Reranking**\n",
    "\n",
    "We use the fast BPR matrix multiplication to generate a \"Candidate Pool\" of 100 items, and then apply the CB model to rerank only those 100 items for the final top-10 display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CandidateRerankHybrid:\n",
    "\n",
    "    def __init__(self, bpr_model, cb_model, n_candidates=100, n_items=None):\n",
    "        self.bpr = bpr_model\n",
    "        self.cb = cb_model\n",
    "        self.n_candidates = n_candidates\n",
    "        self.n_items = n_items\n",
    "    \n",
    "    def predict_for_user(self, user_id, k=10, train_df=None):\n",
    "\n",
    "        candidates = self.bpr.predict_for_user(user_id, k=self.n_candidates, train_df=train_df)\n",
    "        if not candidates:\n",
    "            return []\n",
    "        \n",
    "\n",
    "        if user_id not in self.cb.user_profiles:\n",
    "            return candidates[:k]\n",
    "        \n",
    "        profile = self.cb.user_profiles[user_id]\n",
    "        \n",
    "        reranked = []\n",
    "        for iid, bpr_score in candidates:\n",
    "            if iid < len(self.cb.normalized_features):\n",
    "                cb_score = float(self.cb.normalized_features[iid] @ profile)\n",
    "            else:\n",
    "                cb_score = 0.0\n",
    "\n",
    "            combined = 0.6 * bpr_score + 0.4 * cb_score\n",
    "            reranked.append((iid, combined))\n",
    "        \n",
    "        reranked.sort(key=lambda x: x[1], reverse=True)\n",
    "        return reranked[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPR-MF - Evaluation results\n",
      "Ranking metrics:\n",
      "NDCG@ 5: 0.0549\n",
      "NDCG@10: 0.0649\n",
      "NDCG@20: 0.0822\n",
      "\n",
      "\n",
      "Relevance metrics (threshold=4.0):\n",
      "Recall@ 5: 0.0375\n",
      "Precision@ 5: 0.0498\n",
      "Recall@10: 0.0689\n",
      "Precision@10: 0.0455\n",
      "Recall@20: 0.1208\n",
      "Precision@20: 0.0406\n",
      "\n",
      "\n",
      "Diversity metrics:\n",
      "Coverage: 0.4580\n",
      "Popularity bias: 1094.21\n",
      "Content-Based - Evaluation results\n",
      "Ranking metrics:\n",
      "NDCG@ 5: 0.0304\n",
      "NDCG@10: 0.0359\n",
      "NDCG@20: 0.0448\n",
      "\n",
      "\n",
      "Relevance metrics (threshold=4.0):\n",
      "Recall@ 5: 0.0230\n",
      "Precision@ 5: 0.0255\n",
      "Recall@10: 0.0382\n",
      "Precision@10: 0.0224\n",
      "Recall@20: 0.0626\n",
      "Precision@20: 0.0188\n",
      "\n",
      "\n",
      "Diversity metrics:\n",
      "Coverage: 0.5499\n",
      "Popularity bias: 734.40\n"
     ]
    }
   ],
   "source": [
    "evaluator = RecommenderEvaluator(train, test, k_values=[5, 10, 20])\n",
    "\n",
    "metrics_bpr = evaluator.evaluate_model(bpr, model_name=\"BPR-MF\")\n",
    "evaluator.print_metrics(metrics_bpr, \"BPR-MF\")\n",
    "\n",
    "metrics_cb = evaluator.evaluate_model(cb, model_name=\"Content-Based\")\n",
    "evaluator.print_metrics(metrics_cb, \"Content-Based\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blending weight `alpha` dictates how much we trust the collaborative signal vs content signal. We must tune this on the `val` set to avoid overfitting the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha=0.3: NDCG@10=0.0642, Recall@10=0.0750\n",
      "alpha=0.5: NDCG@10=0.0720, Recall@10=0.0840\n",
      "alpha=0.6: NDCG@10=0.0755, Recall@10=0.0887\n",
      "alpha=0.7: NDCG@10=0.0785, Recall@10=0.0914\n",
      "alpha=0.8: NDCG@10=0.0811, Recall@10=0.0938\n",
      "alpha=0.9: NDCG@10=0.0805, Recall@10=0.0922\n",
      "иest alpha: 0.8\n"
     ]
    }
   ],
   "source": [
    "val_evaluator = RecommenderEvaluator(train, val, k_values=[10])\n",
    "alphas = [0.3, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "alpha_results = []\n",
    "\n",
    "for a in alphas:\n",
    "    model = WeightedBlendHybrid(bpr, cb, alpha=a, n_items=n_items)\n",
    "    m = val_evaluator.evaluate_model(model, model_name=f\"Blend_a{a}\")\n",
    "    alpha_results.append({\"alpha\": a, \"NDCG@10\": m['NDCG@10'], \"Recall@10\": m['Recall@10']})\n",
    "    print(f\"alpha={a:.1f}: NDCG@10={m['NDCG@10']:.4f}, Recall@10={m['Recall@10']:.4f}\")\n",
    "\n",
    "best_alpha = max(alpha_results, key=lambda x: x['NDCG@10'])['alpha']\n",
    "print(f\"best alpha: {best_alpha}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "low alpha means that we rate CB more and it results is worse performance. it means that CB is generally weaker predictor of user tastes than BPR\n",
    "\n",
    "increasing alpha results in better metrics. it peaks with alpha = 0.8\n",
    "\n",
    "increasing alpha even more to 0.9 results in worse metrics. this means that we can't just use 100% BPR and still use some CB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blend(a=0.8) - Evaluation results\n",
      "Ranking metrics:\n",
      "NDCG@ 5: 0.0551\n",
      "NDCG@10: 0.0657\n",
      "NDCG@20: 0.0830\n",
      "\n",
      "\n",
      "Relevance metrics (threshold=4.0):\n",
      "Recall@ 5: 0.0388\n",
      "Precision@ 5: 0.0496\n",
      "Recall@10: 0.0708\n",
      "Precision@10: 0.0452\n",
      "Recall@20: 0.1225\n",
      "Precision@20: 0.0396\n",
      "\n",
      "\n",
      "Diversity metrics:\n",
      "Coverage: 0.4806\n",
      "Popularity bias: 1058.23\n",
      "CandGen+Rerank - Evaluation results\n",
      "Ranking metrics:\n",
      "NDCG@ 5: 0.0555\n",
      "NDCG@10: 0.0655\n",
      "NDCG@20: 0.0827\n",
      "\n",
      "\n",
      "Relevance metrics (threshold=4.0):\n",
      "Recall@ 5: 0.0384\n",
      "Precision@ 5: 0.0503\n",
      "Recall@10: 0.0701\n",
      "Precision@10: 0.0455\n",
      "Recall@20: 0.1219\n",
      "Precision@20: 0.0403\n",
      "\n",
      "\n",
      "Diversity metrics:\n",
      "Coverage: 0.4724\n",
      "Popularity bias: 1066.91\n"
     ]
    }
   ],
   "source": [
    "# evaluating strategies on unseen test and compare to underlying base models\n",
    "\n",
    "blend = WeightedBlendHybrid(bpr, cb, alpha=best_alpha, n_items=n_items)\n",
    "rerank = CandidateRerankHybrid(bpr, cb, n_candidates=100, n_items=n_items)\n",
    "\n",
    "metrics_blend = evaluator.evaluate_model(blend, model_name=f\"Blend(a={best_alpha})\")\n",
    "evaluator.print_metrics(metrics_blend, f\"Blend(a={best_alpha})\")\n",
    "\n",
    "metrics_rerank = evaluator.evaluate_model(rerank, model_name=\"CandGen+Rerank\")\n",
    "evaluator.print_metrics(metrics_rerank, \"CandGen+Rerank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Model  NDCG@5  NDCG@10  NDCG@20  Recall@10  Precision@10  Coverage  Popularity_Bias\n",
      "        BPR-MF  0.0549   0.0649   0.0822     0.0689        0.0455    0.4580        1094.2085\n",
      " Content-Based  0.0304   0.0359   0.0448     0.0382        0.0224    0.5499         734.4027\n",
      "  Blend(a=0.8)  0.0551   0.0657   0.0830     0.0708        0.0452    0.4806        1058.2326\n",
      "CandGen+Rerank  0.0555   0.0655   0.0827     0.0701        0.0455    0.4724        1066.9107\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(evaluator.history)\n",
    "cols = ['Model', 'NDCG@5', 'NDCG@10', 'NDCG@20', 'Recall@10', 'Precision@10', 'Coverage', 'Popularity_Bias']\n",
    "print(results_df[[c for c in cols if c in results_df.columns]].round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* both hybrid models outperform pure base models in accuracy, meaning that they complement each other. CB helps BPR in niche cases with few ratings\n",
    "\n",
    "* Blend is slithly more accurate and has slightly better coverage than CandGen+Rerank. However scoring many items is computationally expensive and reranking allows to run on 100 items and performance drop-off is negligible compared to full Blend\n",
    "\n",
    "* pure CB has 55% coverage and low pop bias of 734. by blending 20% of CB, hybrid gets some of that diversity, having increased its coverate by 2% from 46 to 48% and reduced pop bias from 1100 to 1060 without accuracy sacrifice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold users (<=30 ratings): 1254\n",
      "Warm users (>=100 ratings): 2453\n",
      "         Model  Cold (<=30)  Warm (>=100)\n",
      "        BPR-MF       0.0730        0.0707\n",
      " Content-Based       0.0478        0.0312\n",
      "  Blend(a=0.8)       0.0806        0.0668\n",
      "CandGen+Rerank       0.0769        0.0693\n"
     ]
    }
   ],
   "source": [
    "# checking who benefits: cold or warm start users\n",
    "\n",
    "user_activity = train.groupby('user_id').size()\n",
    "cold_users = set(user_activity[user_activity <= 30].index)\n",
    "warm_users = set(user_activity[user_activity >= 100].index)\n",
    "\n",
    "print(f\"Cold users (<=30 ratings): {len(cold_users)}\")\n",
    "print(f\"Warm users (>=100 ratings): {len(warm_users)}\")\n",
    "\n",
    "def segment_ndcg(model, test_df, train_df, user_set, k=10):\n",
    "    gt = defaultdict(dict)\n",
    "    for _, row in test_df.iterrows():\n",
    "        if row['user_id'] in user_set:\n",
    "            gt[row['user_id']][row['item_id']] = row['rating']\n",
    "    ndcgs = []\n",
    "    for uid, truth in gt.items():\n",
    "        preds = model.predict_for_user(uid, k=k, train_df=train_df)\n",
    "        rel = [truth.get(iid, 0) for iid, _ in preds[:k]]\n",
    "        ideal = sorted(truth.values(), reverse=True)\n",
    "        def dcg(s, k):\n",
    "            s = np.array(s)[:k]\n",
    "            return np.sum((2**s - 1) / np.log2(np.arange(2, len(s)+2))) if len(s) > 0 else 0.0\n",
    "        d, id_ = dcg(rel, k), dcg(ideal, k)\n",
    "        ndcgs.append(d / id_ if id_ > 0 else 0.0)\n",
    "    return np.mean(ndcgs) if ndcgs else 0.0\n",
    "\n",
    "models = {\"BPR-MF\": bpr, \"Content-Based\": cb, f\"Blend(a={best_alpha})\": blend, \"CandGen+Rerank\": rerank}\n",
    "segments = {\"Cold (<=30)\": cold_users, \"Warm (>=100)\": warm_users}\n",
    "\n",
    "user_seg_results = []\n",
    "for mname, model in models.items():\n",
    "    row = {\"Model\": mname}\n",
    "    for sname, uset in segments.items():\n",
    "        row[sname] = segment_ndcg(model, test, train, uset, k=10)\n",
    "    user_seg_results.append(row)\n",
    "\n",
    "user_seg_df = pd.DataFrame(user_seg_results)\n",
    "print(user_seg_df.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* for cold-start users with less than 30 ratings hybrid approach is clear winner, significantly beating pure models. BPR struggles here and falling back to CB allows to get better results\n",
    "* for warm-start users with many ratings pure model actually beats hybrid. We already have information on them and CB is generic, thus adding 20% of it actually only adds noise and dilutes collaborative predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.save_results(\"../experiments/results/hybrid_comparison.csv\")\n",
    "user_seg_df.to_csv(\"../experiments/results/cold_warm_analysis.csv\", index=False)\n",
    "pd.DataFrame(alpha_results).to_csv(\"../experiments/results/alpha_tuning.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
